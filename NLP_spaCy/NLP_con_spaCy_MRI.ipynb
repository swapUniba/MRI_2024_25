{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mV4-EgXGDAA7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing con spaCy\n",
        "**spaCy** è una delle librerie più avanzate e popolari per il Natural Language Processing (NLP) in Python. Creata per essere veloce, scalabile e facile da usare, spaCy offre strumenti di alto livello per l'analisi e la comprensione del linguaggio naturale.\n",
        "\n",
        "Installiamo spaCy."
      ],
      "metadata": {
        "id": "rzSK7jURIeR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.8.2"
      ],
      "metadata": {
        "id": "wVN5yQOkKVXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In spaCy, un modello contiene tutte le informazioni necessarie per eseguire task specifici di NLP. Ogni modello è una pipeline addestrata in grado di elaborare il testo in una determinata lingua e fornire risultati strutturati.\n",
        "\n",
        "spaCy offre pipeline per molte lingue diverse e con diversa grandezza, velocità, e accuratezza.\n",
        "\n",
        "Per poter utilizzare una pipeline abbiamo bisogno di scaricare il relativo modello, proviamo a scaricare il modello `en_core_web_lg`.\n",
        "\n",
        "https://spacy.io/models"
      ],
      "metadata": {
        "id": "f3Y-7qm8Ka2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "HSLpsWUiKcXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per utilizzare il modello scaricato dobbiamo caricarlo usando `spacy.load`. Questo ci restituirà una pipeline, che solitamente chiamiamo `nlp`, contenente tutte le componenti e le informazioni necessarie a processare il testo."
      ],
      "metadata": {
        "id": "eeEzzRQQKg_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "EN6qQp2QKjZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vediamo quali componenti contiene la nostra pipeline.\n",
        "https://spacy.io/usage/processing-pipelines"
      ],
      "metadata": {
        "id": "0qpGhk4Xo6VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "id": "cR-HyfqPoTil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizzazione\n",
        "La tokenizzazione è il primo passo nella maggior parte dei processi NLP. Consiste nel suddividere il testo in unità più piccole, chiamate token. Questi possono essere parole, frasi o persino caratteri. È una tecnica fondamentale perché la maggior parte delle analisi linguistiche richiede di lavorare con unità discrete del testo, piuttosto che con interi blocchi.\n",
        "\n",
        "Tokenizzare un testo potrebbe sembrare un task banale, ma in realtà vengono applicate regole specifiche per ogni lingua. Ad esempio, la punteggiatura va normalmente separata dalle parole - ma nel caso di \"U.K.\" questo deve rimanere un token singolo."
      ],
      "metadata": {
        "id": "7XYquTf3IxUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passando una stringa di testo alla pipeline `nlp` ci viene restituito un documento processato, di classe `Doc` che chiamiamo `doc`. `doc` è una sequenza di token, ossia di oggetti di classe `Token`, che hanno diversi attributi.\n",
        "\n",
        "Per vedere i token che compongono il nostro testo originale ci basta iterare su `doc` e stampare i token."
      ],
      "metadata": {
        "id": "JULnG8jELLzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello World!\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "JXv7I2v4JyLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nonostante `doc` sia processato, contiene ancora tutte le informazioni del testo originale.\n",
        "\n",
        "Possiamo, ad esempio, ricostruire il testo originale accedendo all'attributo `.text_with_ws` che conserva il testo del token insieme agli spazi adiacenti originali."
      ],
      "metadata": {
        "id": "GTQ4qC_FLno2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join([token.text_with_ws for token in doc])"
      ],
      "metadata": {
        "id": "kmv_AusZLovf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possiamo anche recuperare la posizione originale di ogni token all'interno del testo usando l'attributo `idx` contenente l'indice del token."
      ],
      "metadata": {
        "id": "1nQK6gP7LuMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, Index: {token.idx}\")"
      ],
      "metadata": {
        "id": "4GA9N72aLvad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In generale, ogni token possiede diversi attributi:\n",
        "\n",
        "\n",
        "* `.text`: il token così come compare nel testo\n",
        "* `.idx`: l'indice del token all'interno del testo\n",
        "* `.lemma_`: il token nella sua forma canonica, quella che troveremmo all'interno del dizionario\n",
        "* `.is_punct`: il token è un segno di punteggiatura?\n",
        "* `.is_space`: il token è uno spazio?\n",
        "* `.shape_`: trasforma il token per mostrare le sue caratteristiche ortografiche\n",
        "* `pos_`: Universal POS tag (https://universaldependencies.org/u/pos/) del token, specifica il ruolo del token dal punto di vista grammaticale\n",
        "* `tag_`: Penn Treebank POS tag (https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), più specifico degli Universal POS tag. Ad esempio, questo tipo di POS tag è in grado di distinguere il tempo verbale e i nomi al singolare e al plurale\n"
      ],
      "metadata": {
        "id": "6fH-AKPoLxkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"On January 12th, I will be in the U.K.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
        "        token.text,\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "        token.shape_,\n",
        "        token.pos_,\n",
        "        token.tag_\n",
        "    ))"
      ],
      "metadata": {
        "id": "_z_iRquLL11F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Segmentation\n",
        "Il processo di sentence segmentation consiste nell'identificare i confini tra le frasi in un testo continuo, in modo da poterle dividere e analizzare singolarmente."
      ],
      "metadata": {
        "id": "pO7zXm_jK0xM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe `Doc` ci consente di accedere alle singole frasi che compongono il testo.\n",
        "\n",
        "Per farlo, ci basta iterare sull'attributo `.sents`."
      ],
      "metadata": {
        "id": "Zm78BwxrL8R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sentence. This is another sentence.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for i, sent in enumerate(doc.sents):\n",
        "    print(f\"Sentence {i}: {sent}\")"
      ],
      "metadata": {
        "id": "HJbRlpW8K5P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rimozione di caratteri speciali e stopword\n",
        "La rimozione dei caratteri speciali e delle stopword (parole molto frequenti ma poco informative, come \"il\", \"e\", \"di\") è un passaggio di pulizia del testo. Serve a ridurre il rumore nei dati, concentrandosi sulle parole che portano più informazioni utili per l'analisi."
      ],
      "metadata": {
        "id": "8eb_hhVWI1sP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partiamo dalla rimozione dei caratteri speciali.\n",
        "\n",
        "Possiamo filtrare i caratteri speciali utilizzando le proprietà di ogni token. I caratteri speciali come punteggiatura, numeri o caratteri non alfanumerici possono essere infatti filtrati sfruttando i seguenti attributi dei token:\n",
        "* `.is_punct`: per identificare la punteggiatura.\n",
        "* `.is_alpha`: per identificare le parole composte da caratteri alfabetici.\n",
        "* `.is_digit`: per identificare i numeri.\n",
        "* `.is_space`: per identificare gli spazi."
      ],
      "metadata": {
        "id": "H-PjHuZfYzRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Sample text with numbers (123), symbols @$%^&* and punctuation :;,.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "clean_text = [token.text for token in doc if token.is_alpha]\n",
        "print(\"Tokens without numbers, symbols, and punctuations:\", clean_text)"
      ],
      "metadata": {
        "id": "NZUYN9MYZEnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questo modo abbiamo ottenuto una lista di token. Se vogliamo ricostruire il testo mantenendo solo i token alfabetici possiamo usare `token.text_with_ws` come abbiamo fatto in precedenza e filtrare come abbiamo appena visto."
      ],
      "metadata": {
        "id": "tC4UztbPaf14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join([token.text_with_ws for token in doc if token.is_alpha])"
      ],
      "metadata": {
        "id": "hjQ0yR3laib3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La rimozione delle stopword consiste nel rimuovere tutte quelle parole molto comuni in una determinata lingua. Si basa sull'idea che queste parole molto comuni non siano utili all'analisi del testo e si possono quindi rimuovere per ridurre il rumore."
      ],
      "metadata": {
        "id": "YmyGkOr-cAu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The movie I saw last night was not that good\"\n",
        "doc = nlp(text)\n",
        "\n",
        "clean_text = [token.text for token in doc if not token.is_stop]\n",
        "print(\"Tokens without stopwords:\", clean_text)"
      ],
      "metadata": {
        "id": "cpuFb2s3cA-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "È importante notare che non esiste una vera e propria definizione di cosa sia una stopword, per questo motivo non esiste una singola lista universale di stopword per una determinata lingua. Le stopword più opportune da utilizzare variano di caso in caso, per questo motivo è possibile personalizzarle.\n",
        "\n",
        "Vediamo come aggiungere e rimuovere stopword."
      ],
      "metadata": {
        "id": "XniImvPxeDve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggiunta di una stopword\n",
        "nlp.vocab[\"saw\"].is_stop = True\n",
        "\n",
        "# Rimozione di una stopword\n",
        "nlp.vocab[\"not\"].is_stop = False"
      ],
      "metadata": {
        "id": "jZf5cG9mfmYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vediamo come cambia il risultato con le nuove stopword."
      ],
      "metadata": {
        "id": "p2F8CXdHgWAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The movie I saw last night was not that good\"\n",
        "doc = nlp(text)\n",
        "\n",
        "clean_text = [token.text for token in doc if not token.is_stop]\n",
        "print(\"Tokens without stopwords:\", clean_text)"
      ],
      "metadata": {
        "id": "G-Pk9p5pgUho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come prima, possiamo ricostruire il testo originale senza le stopword."
      ],
      "metadata": {
        "id": "vBpjSGiicyQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join([token.text_with_ws for token in doc if not token.is_stop])"
      ],
      "metadata": {
        "id": "yxTlJetIc3Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatizzazione e Stemming\n",
        "La lemmatizzazione e lo stemming sono tecniche per ridurre le parole alle loro forme base.\n",
        "\n",
        "Lo stemming taglia il suffisso delle parole per ottenere una radice comune (es. \"correndo\" -> \"corr\").\n",
        "La lemmatizzazione restituisce la forma grammaticale corretta, basandosi sul contesto (es. \"correndo\" -> \"correre\").\n",
        "Sono utili per analisi testuali più accurate e per uniformare le varianti di una stessa parola."
      ],
      "metadata": {
        "id": "hst7eO8HI-VG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partiamo dalla lemmatizzazione. Come abbiamo visto in precedenza, possiamo accedere al lemma corrispondente ad ogni token grazie all'attributo `.lemma_`."
      ],
      "metadata": {
        "id": "uO7JfZK5jHez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The boys ran quickly through the park while watching the dogs play.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(\"Lemmas:\", lemmas)"
      ],
      "metadata": {
        "id": "jEFoxHncJzqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per quanto riguarda lo stemming, questo non è nativamente supportato da spaCy. Per questo motivo ricorriamo ad uno stemmer presente in un'altra libreria per il Natural Language Processing, chiamata **Natural Language Toolkit** (**NLTK**).\n",
        "\n",
        "Installiamo NLTK."
      ],
      "metadata": {
        "id": "m5NuiGO8jffa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "XkcnFDsijxX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proviamo il Porter Stemmer."
      ],
      "metadata": {
        "id": "M5o4ifUNmAMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stems = [stemmer.stem(token.text) for token in doc]\n",
        "print(\"Stems:\", stems)"
      ],
      "metadata": {
        "id": "dTg-GjUSkHwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech (POS) Tagging\n",
        "Il POS tagging assegna a ogni parola il suo ruolo grammaticale, come sostantivo, verbo o aggettivo. Questa analisi permette di capire meglio il significato delle parole e le relazioni tra di esse, rendendola una base fondamentale per molte tecniche avanzate, come la comprensione del contesto."
      ],
      "metadata": {
        "id": "z8HQQkfsJBT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe `Doc` ci consente di accedere ai POS tag del testo. Come abbiamo visto in precedenza, possiamo usare gli attributi `pos_` e `tag_` dei singoli token per accedere agli Universal POS tag e ai Penn Treebank POS tag."
      ],
      "metadata": {
        "id": "3DjLkhnqMP7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print([(token.text, token.pos_) for token in doc])\n",
        "print([(token.text, token.tag_) for token in doc])"
      ],
      "metadata": {
        "id": "5KH1yIr5J0Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition (NER)\n",
        "La NER identifica e classifica entità nominate in un testo, come persone, luoghi, aziende, date o valori numerici. Questa tecnica è cruciale per estrarre informazioni strutturate da testi non strutturati, con applicazioni in campi come il monitoraggio delle notizie o l'analisi di recensioni.\n",
        "\n",
        "Le entità che possiamo riconoscere in spaCy sono:\n",
        "* PERSON:      People, including fictional.\n",
        "* NORP:        Nationalities or religious or political groups.\n",
        "* FAC:         Buildings, airports, highways, bridges, etc.\n",
        "* ORG:         Companies, agencies, institutions, etc.\n",
        "* GPE:         Countries, cities, states.\n",
        "* LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
        "* PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
        "* EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
        "* WORK_OF_ART: Titles of books, songs, etc.\n",
        "* LAW:         Named documents made into laws.\n",
        "* LANGUAGE:    Any named language.\n",
        "* DATE:        Absolute or relative dates or periods.\n",
        "* TIME:        Times smaller than a day.\n",
        "* PERCENT:     Percentage, including ”%“.\n",
        "* MONEY:       Monetary values, including unit.\n",
        "* QUANTITY:    Measurements, as of weight or distance.\n",
        "* ORDINAL:     “first”, “second”, etc.\n",
        "* CARDINAL:    Numerals that do not fall under another type."
      ],
      "metadata": {
        "id": "kNE-BZmQJG1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe `Doc` ci consente di accedere alle entità riconosciute all'interno del nostro testo. Le entità rilevate nel testo sono contenute nell'attributo `.ents`.\n",
        "\n",
        "Per accedere all'etichetta di un'entità si può utilizzare l'attributo `.label_`.\n",
        "Per accedere all'indice di inizio e di fine di ogni entità si possono usare gli attributi `.start_char` ed `.end_char`."
      ],
      "metadata": {
        "id": "ji3kn2iIMuS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"On January 12th, I will be in Rome.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity text: {ent.text} \\t Label: {ent.label_} \\t Start index: {ent.start_char}  \\t End index: {ent.end_char}\")"
      ],
      "metadata": {
        "id": "Ozd-R3WyJ0qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tag IOB (Inside, Outside, Beginning) sono un sistema utilizzato per annotare le entità nominate ed indicano la posizione di ciascun token all'interno di un'entità nominata.\n",
        "\n",
        "Possiamo accedere ai tag IOB dei token grazie all'attributo `.ent_iob_`."
      ],
      "metadata": {
        "id": "QUgPMZxNMyK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Next week I'll be in Madrid.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "iob_tagged = [\n",
        "    (\n",
        "        token.text,\n",
        "        token.tag_,\n",
        "        \"{0}-{1}\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_\n",
        "    ) for token in doc\n",
        "]\n",
        "print(iob_tagged)"
      ],
      "metadata": {
        "id": "uxBe_UGXMyim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ogni modello disponibile in spaCy è in grado di riconoscere determinati tipi di entità. È possibile vedere la lista completa di entità supportate da ogni modello qui https://spacy.io/models/en."
      ],
      "metadata": {
        "id": "22n4VJC4M5fW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grazie a displaCy possiamo visualizzare grafici e diagrammi che rappresentano visivamente le informazioni linguistiche. È uno strumento interattivo che può essere utilizzato per visualizzare i risultati del processing NLP in modo intuitivo, aiutando nella comprensione delle relazioni tra le parole in una frase e dei concetti estratti dal testo."
      ],
      "metadata": {
        "id": "soazu57FM_Lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usiamo displaCy per visualizzare le entità riconosciute.\n",
        "* `displacy.render` è il metodo che genera la visualizzazione.\n",
        "* `doc` è l'oggetto di classe `Doc` che contiene il testo elaborato.\n",
        "* `style='ent'` indica che vogliamo visualizzare le entità nominate nel testo.\n",
        "* `jupyter=True` permette la visualizzazione all'interno di un Jupyter Notebook."
      ],
      "metadata": {
        "id": "2nooXB7CNBjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "text = \"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days, according to the WSJ\"\n",
        "doc = nlp(text)\n",
        "\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "metadata": {
        "id": "MlqTIfaaNDR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estrazione Noun Chunks\n",
        "Un noun chunk è un gruppo di parole che ruotano attorno a un sostantivo, spesso includendo l'aggettivo, e altre parole che modificano il sostantivo."
      ],
      "metadata": {
        "id": "GLesReO5OmDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe `Doc` ci consente di accedere ai noun chunks individuati nel nostro testo.\n",
        "\n",
        "Possiamo accedere ai noun chunks riconosciuti nel testo tramite l'attributo `.noun_chunks`. Per ogni noun chunk stampiamo il testo grazie all'attributo `.text`, e la \"root\" - cioè il sostantivo principale da cui dipendono gli altri componenti del noun chunk - grazie all'attributo .`root_text`."
      ],
      "metadata": {
        "id": "T8wnbTS3OzUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Wall Street Journal just published an interesting piece on large language models\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)\n",
        "    print(f\"Root: {chunk.root.text} \\n\")"
      ],
      "metadata": {
        "id": "4t6ieQAwOo2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Parsing\n",
        "Il dependency parsing analizza la struttura sintattica di una frase, identificando le relazioni grammaticali tra le parole. Ad esempio, collega un verbo al suo soggetto o oggetto. Questa analisi è fondamentale per comprendere il significato profondo di un testo, soprattutto per applicazioni che richiedono una comprensione avanzata. Ogni parola della frase è associata a un'altra parola chiamata \"head\", e l'intera frase è organizzata come una struttura di dipendenze."
      ],
      "metadata": {
        "id": "vsewaH3OJJ6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possiamo accedere alle informazioni relative al dependency parsing accedendo ai seguenti attributi dei token nella frase:\n",
        "* `token.head.text`: Il testo della head del token. La head è il token principale o centrale da cui il token in esame dipende grammaticalmente.\n",
        "* `token.dep_`: Il ruolo di dipendenza del token nella frase. Indica come il token è grammaticalmente collegato alla sua head.\n",
        "* `token.head.tag_`: Il POS tag della head del token, ovvero il tipo grammaticale della parola centrale da cui il token dipende."
      ],
      "metadata": {
        "id": "Lo8yoWX5Pbso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
        "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
      ],
      "metadata": {
        "id": "rsHxCAqLJ1GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questa visualizzazione non è molto leggibile, possiamo ottenerne una migliore usando ancora displaCy. In questo caso useremo `style='dep'` per indicare che vogliamo stampare l'albero delle dipendenze."
      ],
      "metadata": {
        "id": "sxv0hvRdPlN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "id": "x1jsniyAPu1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analisi del dependency parsing della frase: \"The quick brown fox jumps over the lazy dog\"\n"
      ],
      "metadata": {
        "id": "mV4-EgXGDAA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **\"The\" (det)**\n",
        "   - **Descrizione**: \"The\" è un determinante che precede il sostantivo \"fox\" per specificarlo. In questa frase, \"The\" è un `det` (determinante) e dipende da \"fox\", che è il sostantivo principale del gruppo nominale \"The quick brown fox\".\n",
        "\n",
        "2. **\"quick\" (amod)**\n",
        "   - **Descrizione**: \"quick\" è un aggettivo che modifica il sostantivo \"fox\". Indica una qualità della \"fox\", quindi la relazione di dipendenza tra \"quick\" e \"fox\" è `amod` (modificatore aggettivale). In altre parole, \"quick\" aggiunge una descrizione al sostantivo \"fox\".\n",
        "\n",
        "3. **\"brown\" (amod)**\n",
        "   - **Descrizione**: \"brown\" è un altro aggettivo che modifica \"fox\". Come \"quick\", \"brown\" è anche un modificatore aggettivale che descrive il sostantivo \"fox\". Insieme a \"quick\", forma una descrizione più completa della \"fox\".\n",
        "\n",
        "4. **\"fox\" (nsubj)**\n",
        "   - **Descrizione**: \"fox\" è il soggetto della frase, cioè colui che esegue l'azione del verbo. La relazione di dipendenza di \"fox\" è `nsubj` rispetto al verbo \"jumps\". In altre parole, \"fox\" è il soggetto che compie l'azione di \"jumps\".\n",
        "\n",
        "5. **\"jumps\" (ROOT)**\n",
        "   - **Descrizione**: \"jumps\" è il verbo principale della frase ed è la radice (`root`) della struttura. La radice è il verbo che esprime l'azione centrale della frase. Tutte le altre parole dipendono direttamente o indirettamente da \"jumps\".\n",
        "\n",
        "6. **\"over\" (prep)**\n",
        "   - **Descrizione**: \"over\" è una preposizione che introduce un complemento preposizionale. In questa frase, \"over\" stabilisce una relazione spaziale, dicendo dove la \"fox\" compie l'azione del verbo \"jumps\". La preposizione \"over\" dipende dal verbo \"jumps\", formando una struttura di dipendenza preposizionale.\n",
        "\n",
        "7. **\"the\" (det)**\n",
        "   - **Descrizione**: \"the\" è un determinante che precede il sostantivo \"dog\", specificandolo. In questa frase, \"the\" è un `det` e dipende dal sostantivo \"dog\", che è il complemento dell'azione \"jumps\".\n",
        "\n",
        "8. **\"lazy\" (amod)**\n",
        "   - **Descrizione**: \"lazy\" è un aggettivo che modifica il sostantivo \"dog\". Descrive la qualità del cane, quindi la relazione di dipendenza tra \"lazy\" e \"dog\" è `amod` (modificatore aggettivale). Insieme a \"the\", forma una descrizione completa di \"dog\".\n",
        "\n",
        "9. **\"dog\" (pobj)**\n",
        "   - **Descrizione**: \"dog\" è l'oggetto del complemento preposizionale \"over\". La preposizione \"over\" stabilisce una relazione tra \"fox\" (il soggetto) e \"dog\" (l'oggetto) tramite il movimento sopra di \"dog\". La relazione di dipendenza tra \"dog\" e \"over\" è `pobj` (oggetto preposizionale)."
      ],
      "metadata": {
        "id": "BQ_sw96DDn3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis\n",
        "La sentiment analysis è una tecnica di NLP utilizzata per classificare le emozioni espresse in un testo. Questo processo permette di determinare se il sentiment di un messaggio è positivo, negativo o neutro e di quantificare l'intensità dell'emozione."
      ],
      "metadata": {
        "id": "sc668UcyQrzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dato che spaCy non dispone di strumenti di sentiment analysis, utilizziamo un modello specifico per la sentiment analysis, chiamato **VADER**.\n",
        "\n",
        "Installiamo VADER."
      ],
      "metadata": {
        "id": "5HyRQHsHTcpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment==3.3.2"
      ],
      "metadata": {
        "id": "C3KuyUHsQu2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vediamo ora come utilizzare VADER per ottenere il sentiment di una frase."
      ],
      "metadata": {
        "id": "ifkRY3n2UFJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "text = \"The product quality is excellent, it is a really good product. Unfortunately, I'm disappointed because the delivery was very slow.\"\n",
        "sentiment = sentiment_analyzer.polarity_scores(text)\n",
        "\n",
        "print(\"Sentiment Analysis:\", sentiment)"
      ],
      "metadata": {
        "id": "cftwXTSLTwt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possiamo combinare VADER con la sentence segmentation di spaCy per ottenere il sentiment delle singole frasi nel testo."
      ],
      "metadata": {
        "id": "X_7_1uYCUaV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "\n",
        "for sent in doc.sents:\n",
        "    sent_sentiment = sentiment_analyzer.polarity_scores(sent.text)\n",
        "    print(f\"Sentiment for '{sent.text}': {sent_sentiment}\")"
      ],
      "metadata": {
        "id": "YJVNrCC5UOAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oppure possiamo estendere la classe `Span` fornendole la capacità di fare Sentiment Analysis delle frasi in un testo.\n",
        "\n",
        "Per farlo, dobbiamo aggiungere un nuovo attributo alla classe `Span` che conterrà il sentiment della frase."
      ],
      "metadata": {
        "id": "Z9q5wHUWhBVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(list(doc.sents)[0]))"
      ],
      "metadata": {
        "id": "RfsEga3Xk1DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sentiment_scores(sent):\n",
        "    return sentiment_analyzer.polarity_scores(sent.text)\n",
        "\n",
        "from spacy.tokens import Span\n",
        "Span.set_extension('sentiment_score', getter=compute_sentiment_scores, force=True)"
      ],
      "metadata": {
        "id": "TOt7LmashBoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "  print(f\"Sentiment for '{sent.text}': {sent._.sentiment_score}\")"
      ],
      "metadata": {
        "id": "rpS8tE8ErzNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Componenti Custom: Esempio con WordNet\n",
        "Possiamo creare delle nostre componenti custom per le pipeline, dotate delle funzionalità di cui abbiamo bisogno. È possibile aggiungere queste componenti ad una pipeline esistente."
      ],
      "metadata": {
        "id": "ElWzfovJhQ0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creiamo una componente che ci consenta di estrarre la definizione di una parola usando **WordNet**. WordNet è un dizionario elettronico per la lingua inglese, organizzato in modo da catturare le relazioni semantiche e lessicali tra le parole.\n",
        "\n",
        "Importiamo wordnet da NLTK."
      ],
      "metadata": {
        "id": "ElPyPNZornJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from spacy.tokens import Token\n",
        "from spacy.language import Language"
      ],
      "metadata": {
        "id": "JIujJxfAncdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scarichiamo il dizionario."
      ],
      "metadata": {
        "id": "ENj5HwCFttry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "M7X3F1ilndZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per ottenere la definizione di una parola da WordNet abbiamo bisogno di identificare il suo *synset*. Un synset è un insieme di sinonimi, ogni synset ha quindi un proprio significato ed una propria definizione."
      ],
      "metadata": {
        "id": "S34gvPXGtwep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per assegnare una parola ad un synset abbiamo bisogno di capire in che modo questa viene utilizzata all'interno di una frase, cosa che possiamo fare usando i POS tag. WordNet utilizza un proprio sistema di POS tag, equivalente a quelli Penn Treebank. Dato che la nostra pipeline possiede già la capacità di estrarre questi POS tag, ci basta scrivere una funzione per convertirli."
      ],
      "metadata": {
        "id": "TzhmlQspuu4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to map Penn Treebank POS tags to WordNet POS tags\n",
        "def penn_to_wordnet(tag):\n",
        "    if tag.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif tag.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif tag.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif tag.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    return None  # No matching WordNet POS"
      ],
      "metadata": {
        "id": "lsLT749gnehu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adesso possiamo usare i POS tag di WordNet per ricavare il synset di un token ed estrarre la sua definizione."
      ],
      "metadata": {
        "id": "umJUNieuuym6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love dogs\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    # Convert spaCy's POS tag to WordNet POS tag\n",
        "    wn_pos = penn_to_wordnet(token.tag_)\n",
        "    if wn_pos is None:\n",
        "        continue  # Skip tokens with incompatible POS tags\n",
        "    # Get the first WordNet synset (if available)\n",
        "    synsets = wn.synsets(token.lemma_, wn_pos)\n",
        "    synset = synsets[0]  # Use the first synset\n",
        "    definition = synset.definition(lang=\"eng\")\n",
        "    print(f\"Token: {token.lemma_}\\nDefinition: {definition}\\n\")"
      ],
      "metadata": {
        "id": "I7McC0BWu502"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora, proviamo a creare una componente custom per dare ad una pipeline la capacità di estrarre la definizione di un token.\n",
        "\n",
        "Per farlo creiamo la classe `WordnetDefinition`, la classe rappresenta una componente custom che:\n",
        "* Aggiunge un nuovo attributo custom all'oggetto `Token`, chiamato `definition`.\n",
        "* Processa ogni token nel documento preprocessato per ottenere e conservare la sua definizione."
      ],
      "metadata": {
        "id": "YJ6c0iLxwmoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom spaCy pipeline component to add WordNet synsets\n",
        "class WordnetDefinition:\n",
        "    def __init__(self, nlp):\n",
        "        # Add a custom attribute 'definition' to spaCy's Token object\n",
        "        Token.set_extension('definition', default=None, force=True)\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        for token in doc:\n",
        "            # Convert spaCy's POS tag to WordNet POS tag\n",
        "            wn_pos = penn_to_wordnet(token.tag_)\n",
        "            if wn_pos is None:\n",
        "                continue  # Skip tokens with incompatible POS tags\n",
        "            # Get the first WordNet synset (if available)\n",
        "            synsets = wn.synsets(token.text, wn_pos)\n",
        "            if synsets:\n",
        "                synset = synsets[0]  # Use the first synset\n",
        "                token._.definition = synset.definition(lang=\"eng\")  # Get the definition in English\n",
        "        return doc"
      ],
      "metadata": {
        "id": "wAOFBQkGhRMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per poter aggiungere la nostra nuova componente ad una pipeline di spaCy facendo riferimento al suo nome dobbiamo \"registrare\" la nuova componente. Questo è necessario per dare a spaCy la capacità di creare la nuova componente quando chiediamo di aggiungerla ad una pipeline."
      ],
      "metadata": {
        "id": "EdQWYGo-MOdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the pipeline component in spaCy\n",
        "# This is the function that spaCy calls when you add the \"wordnet_definition\" component to a pipeline\n",
        "@Language.factory(\"wordnet_definition\")\n",
        "def create_wordnet_definition_component(nlp, name):\n",
        "    return WordnetDefinition(nlp)"
      ],
      "metadata": {
        "id": "pTJiPnViMiVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggiungiamo la componente appena creata ad una pipeline e testiamola."
      ],
      "metadata": {
        "id": "Av2IFg1KxNsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.pipeline)"
      ],
      "metadata": {
        "id": "jh05pLJKxYUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"wordnet_definition\", last=True)\n",
        "print(nlp.pipeline)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SUy1euwTxcE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Paris is the awesome capital of France.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.lemma_}\\nDefinition: {token._.definition}\\n\")"
      ],
      "metadata": {
        "id": "-lBGjHtvmMOT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}